{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Normal (Variational) Autoencoder for the Tabula Sapiens Pseudobulks Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import polars as pl\n",
    "from dataset import TS_Compressed_VAE_Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron with ReLU activations and optional BatchNorm.\n",
    "\n",
    "    Careful: if activation is set to ReLU, ReLU is only applied to the second half of NN outputs! \n",
    "            ReLU is applied to standard deviation not mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes,\n",
    "        batch_norm=True,\n",
    "        last_layer_act=\"linear\",\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for s in range(len(sizes) - 1):\n",
    "            layers += [\n",
    "                nn.Linear(sizes[s], sizes[s + 1]),\n",
    "                nn.BatchNorm1d(sizes[s + 1])\n",
    "                if batch_norm and s < len(sizes) - 2\n",
    "                else None,\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "\n",
    "        layers = [l for l in layers if l is not None][:-1]\n",
    "        \n",
    "        self.activation = last_layer_act\n",
    "        if self.activation == \"linear\":\n",
    "            pass\n",
    "        elif self.activation == \"ReLU\":\n",
    "            self.relu = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(\"last_layer_act must be one of 'linear' or 'ReLU'\")\n",
    "\n",
    "        \n",
    "        layers_dict = OrderedDict(\n",
    "                {str(i): module for i, module in enumerate(layers)}\n",
    "            )\n",
    "\n",
    "        self.network = nn.Sequential(layers_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation == \"ReLU\":\n",
    "            x = self.network(x)\n",
    "            dim = x.size(1) // 2\n",
    "            return torch.cat((x[:, :dim], self.relu(x[:, dim:])), dim=1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict()\n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.batch_norm = hparams[\"batch_norm\"]\n",
    "        self.Variational = hparams[\"Variational\"]\n",
    "\n",
    "        if self.Variational:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]*2]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"ReLU\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "        else:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "    def reparametrize(self, mu, sd):\n",
    "        epsilon = torch.randn_like(sd)    \n",
    "        z = mu + sd * epsilon \n",
    "        return z\n",
    "\n",
    "    def get_emb(self, x):\n",
    "        \"\"\"\n",
    "        get the embedding of given expression profiles of genes\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        return self.encoder(x)[:, 0:self.hparams[\"emb_dim\"]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        get the reconstruction of the expression profile of a gene\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        if self.Variational:\n",
    "            mu = latent[:, 0:self.hparams[\"emb_dim\"]]\n",
    "            sd = latent[:, self.hparams[\"emb_dim\"]:]\n",
    "            assert mu.shape == sd.shape\n",
    "            latent = self.reparametrize(mu, sd)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset and Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"TabulaSapiens_pb_normalized.h5ad\"\n",
    "dataset = TS_Compressed_VAE_Dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"dim\": dataset.num_celllines_in_assay(),\n",
    "    \"encoder_width\": 128,\n",
    "    \"encoder_depth\": 5,\n",
    "    \"emb_dim\": 128,\n",
    "    \"decoder_width\": 128,\n",
    "    \"decoder_depth\": 6,\n",
    "    \"batch_norm\": True,\n",
    "    \"Variational\": False,\n",
    "    \n",
    "}\n",
    "config = {\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 1500,\n",
    "    \"lr\": 1e-3\n",
    "}\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkemingzhang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss, batch_size, dataset, epoch):\n",
    "    batch_ct = epoch * dataset.get_num_batches_per_epoch(batch_size)\n",
    "    cumu_loss = 0\n",
    "    for _, target in dataset.get_batches(batch_size, 'train'):\n",
    "        model.to(device)\n",
    "        opt.zero_grad()\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        pred = model(target)\n",
    "        \n",
    "        mse = loss(pred, target)\n",
    "        cumu_loss += mse.item()\n",
    "        mse.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_ct += 1\n",
    "        wandb.log({\"batch_loss\": mse.item(), \"batch_ct\": batch_ct})\n",
    "\n",
    "\n",
    "    #torch.mps.empty_cache()\n",
    "    return cumu_loss / dataset.get_num_batches_per_epoch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_r2(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        target = torch.from_numpy(dataset.test_table[:, 1:].to_numpy().astype('float32')).to(device, non_blocking=True)\n",
    "        pred = model(target).detach().cpu().numpy()\n",
    "        target = target.detach().cpu().numpy()\n",
    "    model.train()\n",
    "    return r2_score(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "  def __init__(self, patience=8, min_delta=-0.1, restore_best_weights=False, restore_app_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.restore_app_weights = restore_app_weights\n",
    "    self.best_model = None\n",
    "    self.app_model = None #the model that may perfrom a bit worse than the best on the test data but better on train data\n",
    "    self.best_r2 = None\n",
    "    self.app_r2 = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "\n",
    "  def __call__(self, model, test_r2):\n",
    "    if self.best_r2 == None:\n",
    "      self.best_r2 = test_r2\n",
    "      self.app_r2 = test_r2\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "      self.app_model = copy.deepcopy(model)\n",
    "\n",
    "    elif test_r2 - self.best_r2 >= 0:\n",
    "      self.best_r2 = test_r2\n",
    "      self.app_r2 = test_r2\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "      self.app_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    elif test_r2 - self.best_r2 >= self.min_delta:\n",
    "      self.counter = 0\n",
    "      self.app_r2 = test_r2\n",
    "      self.app_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    elif test_r2 - self.best_r2 < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_app_weights:\n",
    "          model.load_state_dict(self.app_model.state_dict())\n",
    "        elif self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=config):\n",
    "    with wandb.init(project=\"vae_ts\", config = config):\n",
    "        #this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        model = VAE(hparams)\n",
    "        #es = EarlyStopping()\n",
    "        loss = nn.MSELoss(reduction=\"mean\")\n",
    "        opt = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "\n",
    "\n",
    "        wandb.define_metric(\"batch_loss\", step_metric=\"batch_ct\")\n",
    "        wandb.define_metric(\"avg_loss\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"test_r2\", step_metric=\"epoch\")\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, opt, loss, config.batch_size, dataset, epoch)\n",
    "            wandb.log({\"avg_loss\": avg_loss, \"epoch\": epoch})\n",
    "            test_r2 = eval_r2(model, dataset)\n",
    "            wandb.log({\"test_r2\": test_r2, \"epoch\":epoch})\n",
    "            #if es(model, test_r2): break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zkm/Desktop/tum/genetics/project/functional-gene-embeddings/notebooks/Tabula Sapiens/wandb/run-20240122_231345-98ow57tq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kemingzhang/vae_ts/runs/98ow57tq' target=\"_blank\">elated-brook-21</a></strong> to <a href='https://wandb.ai/kemingzhang/vae_ts' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kemingzhang/vae_ts' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kemingzhang/vae_ts/runs/98ow57tq' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts/runs/98ow57tq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45b4b4886b943a6872d16fb3c9449f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.13352272727272727, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▄▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_ct</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>batch_loss</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_r2</td><td>█▇████████████▇██▆▇██▇█▃█▅█▇██▇█▇██▇███▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.03093</td></tr><tr><td>batch_ct</td><td>18000</td></tr><tr><td>batch_loss</td><td>0.02017</td></tr><tr><td>epoch</td><td>499</td></tr><tr><td>test_r2</td><td>-4035919.22502</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-brook-21</strong> at: <a href='https://wandb.ai/kemingzhang/vae_ts/runs/98ow57tq' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts/runs/98ow57tq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240122_231345-98ow57tq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=177, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (16): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU()\n",
       "      (18): Linear(in_features=128, out_features=177, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = dataset.genes_in_assay()\n",
    "gene_scores = torch.from_numpy(dataset.dataset[:, 1:].to_numpy().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=177, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (16): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU()\n",
       "      (18): Linear(in_features=128, out_features=177, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = model.get_emb(gene_scores).detach().cpu().numpy()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.DataFrame(\n",
    "    data = emb,\n",
    "    index = genes,\n",
    "    columns = [f'EMB_{i}' for i in range(hparams[\"emb_dim\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_0</th>\n",
       "      <th>EMB_1</th>\n",
       "      <th>EMB_2</th>\n",
       "      <th>EMB_3</th>\n",
       "      <th>EMB_4</th>\n",
       "      <th>EMB_5</th>\n",
       "      <th>EMB_6</th>\n",
       "      <th>EMB_7</th>\n",
       "      <th>EMB_8</th>\n",
       "      <th>EMB_9</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000265452</th>\n",
       "      <td>-6.919820</td>\n",
       "      <td>5.343982</td>\n",
       "      <td>10.204840</td>\n",
       "      <td>-5.058686</td>\n",
       "      <td>-5.970248</td>\n",
       "      <td>-13.626997</td>\n",
       "      <td>-8.005278</td>\n",
       "      <td>2.334924</td>\n",
       "      <td>8.291081</td>\n",
       "      <td>-11.489544</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.788873</td>\n",
       "      <td>-2.794478</td>\n",
       "      <td>-6.156457</td>\n",
       "      <td>-8.815792</td>\n",
       "      <td>-6.537822</td>\n",
       "      <td>8.457936</td>\n",
       "      <td>-10.383499</td>\n",
       "      <td>11.630306</td>\n",
       "      <td>9.759427</td>\n",
       "      <td>-1.255642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000260063</th>\n",
       "      <td>-5.339058</td>\n",
       "      <td>4.377153</td>\n",
       "      <td>8.083728</td>\n",
       "      <td>-4.016703</td>\n",
       "      <td>-4.803891</td>\n",
       "      <td>-10.884915</td>\n",
       "      <td>-6.794176</td>\n",
       "      <td>1.987034</td>\n",
       "      <td>6.935829</td>\n",
       "      <td>-8.553007</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.121444</td>\n",
       "      <td>-2.508773</td>\n",
       "      <td>-4.429782</td>\n",
       "      <td>-7.239748</td>\n",
       "      <td>-5.213462</td>\n",
       "      <td>7.236993</td>\n",
       "      <td>-8.096043</td>\n",
       "      <td>9.682898</td>\n",
       "      <td>8.137159</td>\n",
       "      <td>-1.237945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000260254</th>\n",
       "      <td>-5.315782</td>\n",
       "      <td>4.363211</td>\n",
       "      <td>8.042725</td>\n",
       "      <td>-4.002351</td>\n",
       "      <td>-4.769751</td>\n",
       "      <td>-10.819091</td>\n",
       "      <td>-6.750141</td>\n",
       "      <td>1.965974</td>\n",
       "      <td>6.901060</td>\n",
       "      <td>-8.503800</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.094487</td>\n",
       "      <td>-2.506976</td>\n",
       "      <td>-4.401450</td>\n",
       "      <td>-7.199893</td>\n",
       "      <td>-5.171790</td>\n",
       "      <td>7.184188</td>\n",
       "      <td>-8.056158</td>\n",
       "      <td>9.621420</td>\n",
       "      <td>8.093013</td>\n",
       "      <td>-1.235950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000228950</th>\n",
       "      <td>-5.323012</td>\n",
       "      <td>4.367267</td>\n",
       "      <td>8.056360</td>\n",
       "      <td>-4.006558</td>\n",
       "      <td>-4.781978</td>\n",
       "      <td>-10.842258</td>\n",
       "      <td>-6.766303</td>\n",
       "      <td>1.974133</td>\n",
       "      <td>6.913198</td>\n",
       "      <td>-8.520170</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.103116</td>\n",
       "      <td>-2.507045</td>\n",
       "      <td>-4.410837</td>\n",
       "      <td>-7.213929</td>\n",
       "      <td>-5.187171</td>\n",
       "      <td>7.203930</td>\n",
       "      <td>-8.069219</td>\n",
       "      <td>9.643794</td>\n",
       "      <td>8.108402</td>\n",
       "      <td>-1.236462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000110536</th>\n",
       "      <td>-5.561466</td>\n",
       "      <td>4.483913</td>\n",
       "      <td>8.529350</td>\n",
       "      <td>-4.139972</td>\n",
       "      <td>-5.232438</td>\n",
       "      <td>-11.683893</td>\n",
       "      <td>-7.368824</td>\n",
       "      <td>2.290424</td>\n",
       "      <td>7.345352</td>\n",
       "      <td>-9.103452</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.394947</td>\n",
       "      <td>-2.488848</td>\n",
       "      <td>-4.749409</td>\n",
       "      <td>-7.718553</td>\n",
       "      <td>-5.768536</td>\n",
       "      <td>7.946247</td>\n",
       "      <td>-8.517799</td>\n",
       "      <td>10.473782</td>\n",
       "      <td>8.655459</td>\n",
       "      <td>-1.241451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000161270</th>\n",
       "      <td>-5.320959</td>\n",
       "      <td>4.365307</td>\n",
       "      <td>8.055573</td>\n",
       "      <td>-4.004800</td>\n",
       "      <td>-4.784202</td>\n",
       "      <td>-10.844629</td>\n",
       "      <td>-6.770431</td>\n",
       "      <td>1.977348</td>\n",
       "      <td>6.914439</td>\n",
       "      <td>-8.518546</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.101426</td>\n",
       "      <td>-2.505679</td>\n",
       "      <td>-4.409843</td>\n",
       "      <td>-7.215448</td>\n",
       "      <td>-5.190890</td>\n",
       "      <td>7.209930</td>\n",
       "      <td>-8.067607</td>\n",
       "      <td>9.648590</td>\n",
       "      <td>8.110155</td>\n",
       "      <td>-1.236302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000225076</th>\n",
       "      <td>-5.327791</td>\n",
       "      <td>4.369851</td>\n",
       "      <td>8.065884</td>\n",
       "      <td>-4.009334</td>\n",
       "      <td>-4.791033</td>\n",
       "      <td>-10.858951</td>\n",
       "      <td>-6.778502</td>\n",
       "      <td>1.980442</td>\n",
       "      <td>6.922038</td>\n",
       "      <td>-8.531300</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.108912</td>\n",
       "      <td>-2.506972</td>\n",
       "      <td>-4.417233</td>\n",
       "      <td>-7.224072</td>\n",
       "      <td>-5.198606</td>\n",
       "      <td>7.218955</td>\n",
       "      <td>-8.078146</td>\n",
       "      <td>9.660419</td>\n",
       "      <td>8.119690</td>\n",
       "      <td>-1.236883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000142794</th>\n",
       "      <td>-5.368519</td>\n",
       "      <td>4.391788</td>\n",
       "      <td>8.145722</td>\n",
       "      <td>-4.032509</td>\n",
       "      <td>-4.865459</td>\n",
       "      <td>-10.998361</td>\n",
       "      <td>-6.878222</td>\n",
       "      <td>2.031758</td>\n",
       "      <td>6.994969</td>\n",
       "      <td>-8.626948</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.158308</td>\n",
       "      <td>-2.505996</td>\n",
       "      <td>-4.472102</td>\n",
       "      <td>-7.308576</td>\n",
       "      <td>-5.293208</td>\n",
       "      <td>7.341596</td>\n",
       "      <td>-8.153842</td>\n",
       "      <td>9.797400</td>\n",
       "      <td>8.212143</td>\n",
       "      <td>-1.239571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000261938</th>\n",
       "      <td>-5.313808</td>\n",
       "      <td>4.362024</td>\n",
       "      <td>8.039355</td>\n",
       "      <td>-4.001160</td>\n",
       "      <td>-4.767064</td>\n",
       "      <td>-10.813755</td>\n",
       "      <td>-6.746733</td>\n",
       "      <td>1.964369</td>\n",
       "      <td>6.898285</td>\n",
       "      <td>-8.499634</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.092216</td>\n",
       "      <td>-2.506829</td>\n",
       "      <td>-4.399058</td>\n",
       "      <td>-7.196678</td>\n",
       "      <td>-5.168484</td>\n",
       "      <td>7.180127</td>\n",
       "      <td>-8.052823</td>\n",
       "      <td>9.616570</td>\n",
       "      <td>8.089529</td>\n",
       "      <td>-1.235829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000149968</th>\n",
       "      <td>-9.027409</td>\n",
       "      <td>6.927211</td>\n",
       "      <td>12.190820</td>\n",
       "      <td>-6.641363</td>\n",
       "      <td>-5.462924</td>\n",
       "      <td>-14.496261</td>\n",
       "      <td>-6.445783</td>\n",
       "      <td>1.049368</td>\n",
       "      <td>8.630924</td>\n",
       "      <td>-14.637761</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.512678</td>\n",
       "      <td>-3.524757</td>\n",
       "      <td>-8.206688</td>\n",
       "      <td>-8.917325</td>\n",
       "      <td>-5.756027</td>\n",
       "      <td>5.990568</td>\n",
       "      <td>-12.829649</td>\n",
       "      <td>10.516883</td>\n",
       "      <td>9.683783</td>\n",
       "      <td>-1.076189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58870 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    EMB_0     EMB_1      EMB_2     EMB_3     EMB_4      EMB_5  \\\n",
       "gene_id                                                                         \n",
       "ENSG00000265452 -6.919820  5.343982  10.204840 -5.058686 -5.970248 -13.626997   \n",
       "ENSG00000260063 -5.339058  4.377153   8.083728 -4.016703 -4.803891 -10.884915   \n",
       "ENSG00000260254 -5.315782  4.363211   8.042725 -4.002351 -4.769751 -10.819091   \n",
       "ENSG00000228950 -5.323012  4.367267   8.056360 -4.006558 -4.781978 -10.842258   \n",
       "ENSG00000110536 -5.561466  4.483913   8.529350 -4.139972 -5.232438 -11.683893   \n",
       "...                   ...       ...        ...       ...       ...        ...   \n",
       "ENSG00000161270 -5.320959  4.365307   8.055573 -4.004800 -4.784202 -10.844629   \n",
       "ENSG00000225076 -5.327791  4.369851   8.065884 -4.009334 -4.791033 -10.858951   \n",
       "ENSG00000142794 -5.368519  4.391788   8.145722 -4.032509 -4.865459 -10.998361   \n",
       "ENSG00000261938 -5.313808  4.362024   8.039355 -4.001160 -4.767064 -10.813755   \n",
       "ENSG00000149968 -9.027409  6.927211  12.190820 -6.641363 -5.462924 -14.496261   \n",
       "\n",
       "                    EMB_6     EMB_7     EMB_8      EMB_9  ...   EMB_118  \\\n",
       "gene_id                                                   ...             \n",
       "ENSG00000265452 -8.005278  2.334924  8.291081 -11.489544  ... -7.788873   \n",
       "ENSG00000260063 -6.794176  1.987034  6.935829  -8.553007  ... -6.121444   \n",
       "ENSG00000260254 -6.750141  1.965974  6.901060  -8.503800  ... -6.094487   \n",
       "ENSG00000228950 -6.766303  1.974133  6.913198  -8.520170  ... -6.103116   \n",
       "ENSG00000110536 -7.368824  2.290424  7.345352  -9.103452  ... -6.394947   \n",
       "...                   ...       ...       ...        ...  ...       ...   \n",
       "ENSG00000161270 -6.770431  1.977348  6.914439  -8.518546  ... -6.101426   \n",
       "ENSG00000225076 -6.778502  1.980442  6.922038  -8.531300  ... -6.108912   \n",
       "ENSG00000142794 -6.878222  2.031758  6.994969  -8.626948  ... -6.158308   \n",
       "ENSG00000261938 -6.746733  1.964369  6.898285  -8.499634  ... -6.092216   \n",
       "ENSG00000149968 -6.445783  1.049368  8.630924 -14.637761  ... -9.512678   \n",
       "\n",
       "                  EMB_119   EMB_120   EMB_121   EMB_122   EMB_123    EMB_124  \\\n",
       "gene_id                                                                        \n",
       "ENSG00000265452 -2.794478 -6.156457 -8.815792 -6.537822  8.457936 -10.383499   \n",
       "ENSG00000260063 -2.508773 -4.429782 -7.239748 -5.213462  7.236993  -8.096043   \n",
       "ENSG00000260254 -2.506976 -4.401450 -7.199893 -5.171790  7.184188  -8.056158   \n",
       "ENSG00000228950 -2.507045 -4.410837 -7.213929 -5.187171  7.203930  -8.069219   \n",
       "ENSG00000110536 -2.488848 -4.749409 -7.718553 -5.768536  7.946247  -8.517799   \n",
       "...                   ...       ...       ...       ...       ...        ...   \n",
       "ENSG00000161270 -2.505679 -4.409843 -7.215448 -5.190890  7.209930  -8.067607   \n",
       "ENSG00000225076 -2.506972 -4.417233 -7.224072 -5.198606  7.218955  -8.078146   \n",
       "ENSG00000142794 -2.505996 -4.472102 -7.308576 -5.293208  7.341596  -8.153842   \n",
       "ENSG00000261938 -2.506829 -4.399058 -7.196678 -5.168484  7.180127  -8.052823   \n",
       "ENSG00000149968 -3.524757 -8.206688 -8.917325 -5.756027  5.990568 -12.829649   \n",
       "\n",
       "                   EMB_125   EMB_126   EMB_127  \n",
       "gene_id                                         \n",
       "ENSG00000265452  11.630306  9.759427 -1.255642  \n",
       "ENSG00000260063   9.682898  8.137159 -1.237945  \n",
       "ENSG00000260254   9.621420  8.093013 -1.235950  \n",
       "ENSG00000228950   9.643794  8.108402 -1.236462  \n",
       "ENSG00000110536  10.473782  8.655459 -1.241451  \n",
       "...                    ...       ...       ...  \n",
       "ENSG00000161270   9.648590  8.110155 -1.236302  \n",
       "ENSG00000225076   9.660419  8.119690 -1.236883  \n",
       "ENSG00000142794   9.797400  8.212143 -1.239571  \n",
       "ENSG00000261938   9.616570  8.089529 -1.235829  \n",
       "ENSG00000149968  10.516883  9.683783 -1.076189  \n",
       "\n",
       "[58870 rows x 128 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.index.name = \"gene_id\"\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = hparams[\"emb_dim\"]\n",
    "emb_df.to_csv(f\"TS_pb_ae_d{emb_dim}.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheet5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
