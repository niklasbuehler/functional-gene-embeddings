{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Normal Variational Autoencoder for the compressed Tabula Sapiens Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import polars as pl\n",
    "from dataset import TS_Compressed_VAE_Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron with ReLU activations and optional BatchNorm.\n",
    "\n",
    "    Careful: if activation is set to ReLU, ReLU is only applied to the second half of NN outputs! \n",
    "            ReLU is applied to standard deviation not mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes,\n",
    "        batch_norm=True,\n",
    "        last_layer_act=\"linear\",\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for s in range(len(sizes) - 1):\n",
    "            layers += [\n",
    "                nn.Linear(sizes[s], sizes[s + 1]),\n",
    "                nn.BatchNorm1d(sizes[s + 1])\n",
    "                if batch_norm and s < len(sizes) - 2\n",
    "                else None,\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "\n",
    "        layers = [l for l in layers if l is not None][:-1]\n",
    "        \n",
    "        self.activation = last_layer_act\n",
    "        if self.activation == \"linear\":\n",
    "            pass\n",
    "        elif self.activation == \"ReLU\":\n",
    "            self.relu = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(\"last_layer_act must be one of 'linear' or 'ReLU'\")\n",
    "\n",
    "        \n",
    "        layers_dict = OrderedDict(\n",
    "                {str(i): module for i, module in enumerate(layers)}\n",
    "            )\n",
    "\n",
    "        self.network = nn.Sequential(layers_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation == \"ReLU\":\n",
    "            x = self.network(x)\n",
    "            dim = x.size(1) // 2\n",
    "            return torch.cat((x[:, :dim], self.relu(x[:, dim:])), dim=1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict()\n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.batch_norm = hparams[\"batch_norm\"]\n",
    "        self.Variational = hparams[\"Variational\"]\n",
    "\n",
    "        if self.Variational:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]*2]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"ReLU\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "        else:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "    def reparametrize(self, mu, sd):\n",
    "        epsilon = torch.randn_like(sd)    \n",
    "        z = mu + sd * epsilon \n",
    "        return z\n",
    "\n",
    "    def get_emb(self, x):\n",
    "        \"\"\"\n",
    "        get the embedding of given expression profiles of genes\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        return self.encoder(x)[:, 0:self.hparams[\"emb_dim\"]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        get the reconstruction of the expression profile of a gene\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        if self.Variational:\n",
    "            mu = latent[:, 0:self.hparams[\"emb_dim\"]]\n",
    "            sd = latent[:, self.hparams[\"emb_dim\"]:]\n",
    "            assert mu.shape == sd.shape\n",
    "            latent = self.reparametrize(mu, sd)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset and Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"TabulaSapiens_CO_compressed.h5ad\"\n",
    "dataset = TS_Compressed_VAE_Dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"dim\": dataset.num_celllines_in_assay(),\n",
    "    \"encoder_width\": 128,\n",
    "    \"encoder_depth\": 4,\n",
    "    \"emb_dim\": 128,\n",
    "    \"decoder_width\": 128,\n",
    "    \"decoder_depth\": 4,\n",
    "    \"batch_norm\": True,\n",
    "    \"Variational\": True,\n",
    "    \n",
    "}\n",
    "config = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 1e3,\n",
    "    \"lr\": 1e-3\n",
    "}\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkemingzhang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss, batch_size, dataset, epoch):\n",
    "    batch_ct = epoch * dataset.get_num_batches_per_epoch(batch_size)\n",
    "    cumu_loss = 0\n",
    "    for _, target in dataset.get_batches(batch_size, 'train'):\n",
    "        model.to(device)\n",
    "        opt.zero_grad()\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        pred = model(target)\n",
    "        \n",
    "        mse = loss(pred, target)\n",
    "        cumu_loss += mse.item()\n",
    "        mse.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_ct += 1\n",
    "        wandb.log({\"batch_loss\": mse.item(), \"batch_ct\": batch_ct})\n",
    "\n",
    "\n",
    "    #torch.mps.empty_cache()\n",
    "    return cumu_loss / dataset.get_num_batches_per_epoch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_r2(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        target = torch.from_numpy(dataset.test_table[:, 1:].to_numpy().astype('float32')).to(device, non_blocking=True)\n",
    "        pred = model(target).detach().cpu().numpy()\n",
    "        target = target.detach().cpu().numpy()\n",
    "    model.train()\n",
    "    return r2_score(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=config):\n",
    "    with wandb.init(project=\"vae_ts\", config = config):\n",
    "        #this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        model = VAE(hparams)\n",
    "\n",
    "        loss = nn.MSELoss(reduction=\"mean\")\n",
    "        opt = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "\n",
    "\n",
    "        wandb.define_metric(\"batch_loss\", step_metric=\"batch_ct\")\n",
    "        wandb.define_metric(\"avg_loss\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"test_r2\", step_metric=\"epoch\")\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, opt, loss, config.batch_size, dataset, epoch)\n",
    "            wandb.log({\"avg_loss\": avg_loss, \"epoch\": epoch})\n",
    "            test_r2 = eval_r2(model, dataset)\n",
    "            wandb.log({\"test_r2\": test_r2, \"epoch\":epoch})\n",
    "\n",
    "\n",
    "        #save the model in the exchangable ONNX format\n",
    "        target = torch.from_numpy(dataset.test_table[:, 1:].to_numpy().astype('float32')).to(device, non_blocking=True)\n",
    "        torch.onnx.export(model, target, \"model.onnx\")\n",
    "        wandb.save(\"model.onnx\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zkm/Desktop/tum/genetics/project/functional-gene-embeddings/notebooks/Tabula Sapiens/wandb/run-20240117_231138-1e1bel2w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kemingzhang/vae_ts/runs/1e1bel2w' target=\"_blank\">distinctive-breeze-16</a></strong> to <a href='https://wandb.ai/kemingzhang/vae_ts' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kemingzhang/vae_ts' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kemingzhang/vae_ts/runs/1e1bel2w' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts/runs/1e1bel2w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/3rtqd2bx3pl1j7g15bs_3jp00000gn/T/ipykernel_11099/21962628.py:44: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert mu.shape == sd.shape\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f03377d101475e92e7ebf842a4af89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.768 MB of 0.768 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▄▃▃▃▂▂▃▄▂▂▂▂▁▂▂▂▁▁▂▂</td></tr><tr><td>batch_ct</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch_loss</td><td>▃▂▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_r2</td><td>▆▃▆▁▇▆▇█▇█▇▁▇▇█▇▇█████▇██████████▇███▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.37985</td></tr><tr><td>batch_ct</td><td>5300</td></tr><tr><td>batch_loss</td><td>0.02249</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>test_r2</td><td>-2000.27312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-breeze-16</strong> at: <a href='https://wandb.ai/kemingzhang/vae_ts/runs/1e1bel2w' target=\"_blank\">https://wandb.ai/kemingzhang/vae_ts/runs/1e1bel2w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240117_231138-1e1bel2w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = dataset.genes_in_assay()\n",
    "gene_scores = torch.from_numpy(dataset.dataset[:, 1:].to_numpy().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): MLP(\n",
       "    (relu): ReLU()\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=177, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=177, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = model.get_emb(gene_scores).detach().cpu().numpy()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.DataFrame(\n",
    "    data = emb,\n",
    "    index = genes,\n",
    "    columns = [f'EMB_{i}' for i in range(hparams[\"emb_dim\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_0</th>\n",
       "      <th>EMB_1</th>\n",
       "      <th>EMB_2</th>\n",
       "      <th>EMB_3</th>\n",
       "      <th>EMB_4</th>\n",
       "      <th>EMB_5</th>\n",
       "      <th>EMB_6</th>\n",
       "      <th>EMB_7</th>\n",
       "      <th>EMB_8</th>\n",
       "      <th>EMB_9</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_118</th>\n",
       "      <th>EMB_119</th>\n",
       "      <th>EMB_120</th>\n",
       "      <th>EMB_121</th>\n",
       "      <th>EMB_122</th>\n",
       "      <th>EMB_123</th>\n",
       "      <th>EMB_124</th>\n",
       "      <th>EMB_125</th>\n",
       "      <th>EMB_126</th>\n",
       "      <th>EMB_127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000268723</th>\n",
       "      <td>-2.447452</td>\n",
       "      <td>-2.578989</td>\n",
       "      <td>1.441438</td>\n",
       "      <td>-1.551750</td>\n",
       "      <td>2.606443</td>\n",
       "      <td>1.790830</td>\n",
       "      <td>-3.063021</td>\n",
       "      <td>1.277864</td>\n",
       "      <td>0.830067</td>\n",
       "      <td>-2.299615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>1.454578</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>-2.255242</td>\n",
       "      <td>3.855302</td>\n",
       "      <td>-1.307074</td>\n",
       "      <td>2.449574</td>\n",
       "      <td>-2.159902</td>\n",
       "      <td>1.629684</td>\n",
       "      <td>-2.412930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000272472</th>\n",
       "      <td>-2.445033</td>\n",
       "      <td>-2.576698</td>\n",
       "      <td>1.440027</td>\n",
       "      <td>-1.550353</td>\n",
       "      <td>2.604102</td>\n",
       "      <td>1.789134</td>\n",
       "      <td>-3.060102</td>\n",
       "      <td>1.276379</td>\n",
       "      <td>0.829069</td>\n",
       "      <td>-2.297524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042579</td>\n",
       "      <td>1.453420</td>\n",
       "      <td>0.509974</td>\n",
       "      <td>-2.252960</td>\n",
       "      <td>3.851781</td>\n",
       "      <td>-1.305450</td>\n",
       "      <td>2.447245</td>\n",
       "      <td>-2.158240</td>\n",
       "      <td>1.628374</td>\n",
       "      <td>-2.410498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000252002</th>\n",
       "      <td>-2.474335</td>\n",
       "      <td>-2.604754</td>\n",
       "      <td>1.456906</td>\n",
       "      <td>-1.567647</td>\n",
       "      <td>2.633294</td>\n",
       "      <td>1.809640</td>\n",
       "      <td>-3.095583</td>\n",
       "      <td>1.294581</td>\n",
       "      <td>0.841056</td>\n",
       "      <td>-2.323334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040572</td>\n",
       "      <td>1.467866</td>\n",
       "      <td>0.513787</td>\n",
       "      <td>-2.280678</td>\n",
       "      <td>3.894751</td>\n",
       "      <td>-1.324960</td>\n",
       "      <td>2.475613</td>\n",
       "      <td>-2.178764</td>\n",
       "      <td>1.644406</td>\n",
       "      <td>-2.440076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000180806</th>\n",
       "      <td>-2.601565</td>\n",
       "      <td>-2.740588</td>\n",
       "      <td>1.539280</td>\n",
       "      <td>-1.651261</td>\n",
       "      <td>2.761152</td>\n",
       "      <td>1.904528</td>\n",
       "      <td>-3.251044</td>\n",
       "      <td>1.383297</td>\n",
       "      <td>0.887083</td>\n",
       "      <td>-2.428858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035545</td>\n",
       "      <td>1.525795</td>\n",
       "      <td>0.536068</td>\n",
       "      <td>-2.408858</td>\n",
       "      <td>4.079382</td>\n",
       "      <td>-1.411488</td>\n",
       "      <td>2.598793</td>\n",
       "      <td>-2.269048</td>\n",
       "      <td>1.715945</td>\n",
       "      <td>-2.569330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000165525</th>\n",
       "      <td>-1.986893</td>\n",
       "      <td>-0.233340</td>\n",
       "      <td>0.212840</td>\n",
       "      <td>-0.884871</td>\n",
       "      <td>3.655952</td>\n",
       "      <td>0.128181</td>\n",
       "      <td>-2.359196</td>\n",
       "      <td>0.913187</td>\n",
       "      <td>1.621459</td>\n",
       "      <td>-3.916359</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.070255</td>\n",
       "      <td>2.006452</td>\n",
       "      <td>-0.443681</td>\n",
       "      <td>-1.327221</td>\n",
       "      <td>4.845542</td>\n",
       "      <td>-0.971142</td>\n",
       "      <td>3.540757</td>\n",
       "      <td>-3.027183</td>\n",
       "      <td>1.400047</td>\n",
       "      <td>-3.174893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000273513</th>\n",
       "      <td>-2.443378</td>\n",
       "      <td>-2.575095</td>\n",
       "      <td>1.439057</td>\n",
       "      <td>-1.549378</td>\n",
       "      <td>2.602487</td>\n",
       "      <td>1.787957</td>\n",
       "      <td>-3.058109</td>\n",
       "      <td>1.275344</td>\n",
       "      <td>0.828405</td>\n",
       "      <td>-2.296116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042658</td>\n",
       "      <td>1.452626</td>\n",
       "      <td>0.509730</td>\n",
       "      <td>-2.251409</td>\n",
       "      <td>3.849388</td>\n",
       "      <td>-1.304365</td>\n",
       "      <td>2.445667</td>\n",
       "      <td>-2.157098</td>\n",
       "      <td>1.627464</td>\n",
       "      <td>-2.408860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000223313</th>\n",
       "      <td>-2.448873</td>\n",
       "      <td>-2.580260</td>\n",
       "      <td>1.442225</td>\n",
       "      <td>-1.552513</td>\n",
       "      <td>2.607687</td>\n",
       "      <td>1.791833</td>\n",
       "      <td>-3.064644</td>\n",
       "      <td>1.278627</td>\n",
       "      <td>0.830607</td>\n",
       "      <td>-2.300694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>1.455190</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>-2.256455</td>\n",
       "      <td>3.857208</td>\n",
       "      <td>-1.307922</td>\n",
       "      <td>2.450789</td>\n",
       "      <td>-2.160752</td>\n",
       "      <td>1.630385</td>\n",
       "      <td>-2.414178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000158623</th>\n",
       "      <td>-2.646688</td>\n",
       "      <td>-2.789868</td>\n",
       "      <td>1.570103</td>\n",
       "      <td>-1.681018</td>\n",
       "      <td>2.802729</td>\n",
       "      <td>1.938740</td>\n",
       "      <td>-3.305467</td>\n",
       "      <td>1.414327</td>\n",
       "      <td>0.901650</td>\n",
       "      <td>-2.462649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>1.542633</td>\n",
       "      <td>0.544556</td>\n",
       "      <td>-2.455058</td>\n",
       "      <td>4.141959</td>\n",
       "      <td>-1.442622</td>\n",
       "      <td>2.640727</td>\n",
       "      <td>-2.298578</td>\n",
       "      <td>1.739371</td>\n",
       "      <td>-2.613292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000284922</th>\n",
       "      <td>-2.443314</td>\n",
       "      <td>-2.575032</td>\n",
       "      <td>1.439018</td>\n",
       "      <td>-1.549340</td>\n",
       "      <td>2.602424</td>\n",
       "      <td>1.787911</td>\n",
       "      <td>-3.058031</td>\n",
       "      <td>1.275303</td>\n",
       "      <td>0.828379</td>\n",
       "      <td>-2.296062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>1.452595</td>\n",
       "      <td>0.509720</td>\n",
       "      <td>-2.251348</td>\n",
       "      <td>3.849296</td>\n",
       "      <td>-1.304322</td>\n",
       "      <td>2.445606</td>\n",
       "      <td>-2.157054</td>\n",
       "      <td>1.627427</td>\n",
       "      <td>-2.408796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000203729</th>\n",
       "      <td>-2.443921</td>\n",
       "      <td>-2.575628</td>\n",
       "      <td>1.439379</td>\n",
       "      <td>-1.549699</td>\n",
       "      <td>2.603016</td>\n",
       "      <td>1.788346</td>\n",
       "      <td>-3.058764</td>\n",
       "      <td>1.275688</td>\n",
       "      <td>0.828621</td>\n",
       "      <td>-2.296574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042638</td>\n",
       "      <td>1.452886</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>-2.251919</td>\n",
       "      <td>3.850173</td>\n",
       "      <td>-1.304723</td>\n",
       "      <td>2.446185</td>\n",
       "      <td>-2.157473</td>\n",
       "      <td>1.627764</td>\n",
       "      <td>-2.409398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58870 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    EMB_0     EMB_1     EMB_2     EMB_3     EMB_4     EMB_5  \\\n",
       "gene_id                                                                       \n",
       "ENSG00000268723 -2.447452 -2.578989  1.441438 -1.551750  2.606443  1.790830   \n",
       "ENSG00000272472 -2.445033 -2.576698  1.440027 -1.550353  2.604102  1.789134   \n",
       "ENSG00000252002 -2.474335 -2.604754  1.456906 -1.567647  2.633294  1.809640   \n",
       "ENSG00000180806 -2.601565 -2.740588  1.539280 -1.651261  2.761152  1.904528   \n",
       "ENSG00000165525 -1.986893 -0.233340  0.212840 -0.884871  3.655952  0.128181   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000273513 -2.443378 -2.575095  1.439057 -1.549378  2.602487  1.787957   \n",
       "ENSG00000223313 -2.448873 -2.580260  1.442225 -1.552513  2.607687  1.791833   \n",
       "ENSG00000158623 -2.646688 -2.789868  1.570103 -1.681018  2.802729  1.938740   \n",
       "ENSG00000284922 -2.443314 -2.575032  1.439018 -1.549340  2.602424  1.787911   \n",
       "ENSG00000203729 -2.443921 -2.575628  1.439379 -1.549699  2.603016  1.788346   \n",
       "\n",
       "                    EMB_6     EMB_7     EMB_8     EMB_9  ...   EMB_118  \\\n",
       "gene_id                                                  ...             \n",
       "ENSG00000268723 -3.063021  1.277864  0.830067 -2.299615  ...  0.042474   \n",
       "ENSG00000272472 -3.060102  1.276379  0.829069 -2.297524  ...  0.042579   \n",
       "ENSG00000252002 -3.095583  1.294581  0.841056 -2.323334  ...  0.040572   \n",
       "ENSG00000180806 -3.251044  1.383297  0.887083 -2.428858  ...  0.035545   \n",
       "ENSG00000165525 -2.359196  0.913187  1.621459 -3.916359  ... -2.070255   \n",
       "...                   ...       ...       ...       ...  ...       ...   \n",
       "ENSG00000273513 -3.058109  1.275344  0.828405 -2.296116  ...  0.042658   \n",
       "ENSG00000223313 -3.064644  1.278627  0.830607 -2.300694  ...  0.042431   \n",
       "ENSG00000158623 -3.305467  1.414327  0.901650 -2.462649  ...  0.036495   \n",
       "ENSG00000284922 -3.058031  1.275303  0.828379 -2.296062  ...  0.042660   \n",
       "ENSG00000203729 -3.058764  1.275688  0.828621 -2.296574  ...  0.042638   \n",
       "\n",
       "                  EMB_119   EMB_120   EMB_121   EMB_122   EMB_123   EMB_124  \\\n",
       "gene_id                                                                       \n",
       "ENSG00000268723  1.454578  0.510296 -2.255242  3.855302 -1.307074  2.449574   \n",
       "ENSG00000272472  1.453420  0.509974 -2.252960  3.851781 -1.305450  2.447245   \n",
       "ENSG00000252002  1.467866  0.513787 -2.280678  3.894751 -1.324960  2.475613   \n",
       "ENSG00000180806  1.525795  0.536068 -2.408858  4.079382 -1.411488  2.598793   \n",
       "ENSG00000165525  2.006452 -0.443681 -1.327221  4.845542 -0.971142  3.540757   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000273513  1.452626  0.509730 -2.251409  3.849388 -1.304365  2.445667   \n",
       "ENSG00000223313  1.455190  0.510514 -2.256455  3.857208 -1.307922  2.450789   \n",
       "ENSG00000158623  1.542633  0.544556 -2.455058  4.141959 -1.442622  2.640727   \n",
       "ENSG00000284922  1.452595  0.509720 -2.251348  3.849296 -1.304322  2.445606   \n",
       "ENSG00000203729  1.452886  0.509813 -2.251919  3.850173 -1.304723  2.446185   \n",
       "\n",
       "                  EMB_125   EMB_126   EMB_127  \n",
       "gene_id                                        \n",
       "ENSG00000268723 -2.159902  1.629684 -2.412930  \n",
       "ENSG00000272472 -2.158240  1.628374 -2.410498  \n",
       "ENSG00000252002 -2.178764  1.644406 -2.440076  \n",
       "ENSG00000180806 -2.269048  1.715945 -2.569330  \n",
       "ENSG00000165525 -3.027183  1.400047 -3.174893  \n",
       "...                   ...       ...       ...  \n",
       "ENSG00000273513 -2.157098  1.627464 -2.408860  \n",
       "ENSG00000223313 -2.160752  1.630385 -2.414178  \n",
       "ENSG00000158623 -2.298578  1.739371 -2.613292  \n",
       "ENSG00000284922 -2.157054  1.627427 -2.408796  \n",
       "ENSG00000203729 -2.157473  1.627764 -2.409398  \n",
       "\n",
       "[58870 rows x 128 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.index.name = \"gene_id\"\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = hparams[\"emb_dim\"]\n",
    "#emb_df.to_csv(f\"TS_compressed_vae_d{emb_dim}.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheet5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
