{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Normal Variational Autoencoder for the DepMap Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import polars as pl\n",
    "from dataset import DepMap_Data\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron with ReLU activations and optional BatchNorm.\n",
    "\n",
    "    Careful: if activation is set to ReLU, ReLU is only applied to the second half of NN outputs! \n",
    "            ReLU is applied to standard deviation not mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes,\n",
    "        batch_norm=True,\n",
    "        last_layer_act=\"linear\",\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for s in range(len(sizes) - 1):\n",
    "            layers += [\n",
    "                nn.Linear(sizes[s], sizes[s + 1]),\n",
    "                nn.BatchNorm1d(sizes[s + 1])\n",
    "                if batch_norm and s < len(sizes) - 2\n",
    "                else None,\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "\n",
    "        layers = [l for l in layers if l is not None][:-1]\n",
    "        \n",
    "        self.activation = last_layer_act\n",
    "        if self.activation == \"linear\":\n",
    "            pass\n",
    "        elif self.activation == \"ReLU\":\n",
    "            self.relu = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(\"last_layer_act must be one of 'linear' or 'ReLU'\")\n",
    "\n",
    "        \n",
    "        layers_dict = OrderedDict(\n",
    "                {str(i): module for i, module in enumerate(layers)}\n",
    "            )\n",
    "\n",
    "        self.network = nn.Sequential(layers_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation == \"ReLU\":\n",
    "            x = self.network(x)\n",
    "            dim = x.size(1) // 2\n",
    "            return torch.cat((x[:, :dim], self.relu(x[:, dim:])), dim=1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict()\n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.batch_norm = hparams[\"batch_norm\"]\n",
    "        self.Variational = hparams[\"Variational\"]\n",
    "\n",
    "        if self.Variational:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]*2]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"ReLU\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "        else:\n",
    "            self.encoder_sizes = [self.hparams[\"dim\"]]+[self.hparams[\"encoder_width\"]]* self.hparams[\"encoder_depth\"]+ [self.hparams[\"emb_dim\"]]\n",
    "            self.decoder_sizes = [self.hparams[\"emb_dim\"]]+[self.hparams[\"decoder_width\"]]* self.hparams[\"decoder_depth\"]+ [self.hparams[\"dim\"]]\n",
    "            self.encoder = MLP(self.encoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "            self.decoder = MLP(self.decoder_sizes, batch_norm=self.batch_norm, last_layer_act=\"linear\")\n",
    "\n",
    "    def reparametrize(self, mu, sd):\n",
    "        epsilon = torch.randn_like(sd)    \n",
    "        z = mu + sd * epsilon \n",
    "        return z\n",
    "\n",
    "    def get_emb(self, x):\n",
    "        \"\"\"\n",
    "        get the embedding of given expression profiles of genes\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        return self.encoder(x)[:, 0:self.hparams[\"emb_dim\"]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        get the reconstruction of the expression profile of a gene\n",
    "        @param x: should be the shape [batch_size, hparams[\"dim]]\n",
    "        \"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        if self.Variational:\n",
    "            mu = latent[:, 0:self.hparams[\"emb_dim\"]]\n",
    "            sd = latent[:, self.hparams[\"emb_dim\"]:]\n",
    "            assert mu.shape == sd.shape\n",
    "            latent = self.reparametrize(mu, sd)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset and Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DepMap18Q3_gene_effect_raw.tsv\"\n",
    "dataset = DepMap_Data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"dim\": dataset.num_celllines_in_assay(),\n",
    "    \"encoder_width\": 256,\n",
    "    \"encoder_depth\": 4,\n",
    "    \"emb_dim\": 256,\n",
    "    \"decoder_width\": 256,\n",
    "    \"decoder_depth\": 4,\n",
    "    \"batch_norm\": True,\n",
    "    \"Variational\": True,\n",
    "    \n",
    "}\n",
    "config = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 1e3,\n",
    "    \"lr\": 1e-3\n",
    "}\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkemingzhang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss, batch_size, dataset, epoch):\n",
    "    batch_ct = epoch * dataset.get_num_batches_per_epoch(batch_size)\n",
    "    cumu_loss = 0\n",
    "    for _, target in dataset.get_batches(batch_size, 'train'):\n",
    "        model.to(device)\n",
    "        opt.zero_grad()\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        pred = model(target)\n",
    "        \n",
    "        mse = loss(pred, target)\n",
    "        cumu_loss += mse.item()\n",
    "        mse.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_ct += 1\n",
    "        wandb.log({\"batch_loss\": mse.item(), \"batch_ct\": batch_ct})\n",
    "\n",
    "\n",
    "    #torch.mps.empty_cache()\n",
    "    return cumu_loss / len(dataset.train_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_r2(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        target = torch.from_numpy(dataset.test_table[:, 1:].to_numpy().astype('float32')).to(device, non_blocking=True)\n",
    "        pred = model(target).detach().cpu().numpy()\n",
    "        target = target.detach().cpu().numpy()\n",
    "    model.train()\n",
    "    return r2_score(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=config):\n",
    "    with wandb.init(project=\"vae_dm\", config = config):\n",
    "        #this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        model = VAE(hparams)\n",
    "\n",
    "        loss = nn.MSELoss(reduction=\"mean\")\n",
    "        opt = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "\n",
    "\n",
    "        wandb.define_metric(\"batch_loss\", step_metric=\"batch_ct\")\n",
    "        wandb.define_metric(\"avg_loss\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"test_r2\", step_metric=\"epoch\")\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, opt, loss, config.batch_size, dataset, epoch)\n",
    "            wandb.log({\"avg_loss\": avg_loss, \"epoch\": epoch})\n",
    "            test_r2 = eval_r2(model, dataset)\n",
    "            wandb.log({\"test_r2\": test_r2, \"epoch\":epoch})\n",
    "\n",
    "\n",
    "        #save the model in the exchangable ONNX format\n",
    "        target = torch.from_numpy(dataset.test_table[:, 1:].to_numpy().astype('float32')).to(device, non_blocking=True)\n",
    "        torch.onnx.export(model, target, \"model.onnx\")\n",
    "        wandb.save(\"model.onnx\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zkm/Desktop/tum/genetics/project/functional-gene-embeddings/notebooks/DepMap/wandb/run-20240117_200801-01hziqur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kemingzhang/vae_dm/runs/01hziqur' target=\"_blank\">spring-feather-24</a></strong> to <a href='https://wandb.ai/kemingzhang/vae_dm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kemingzhang/vae_dm' target=\"_blank\">https://wandb.ai/kemingzhang/vae_dm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kemingzhang/vae_dm/runs/01hziqur' target=\"_blank\">https://wandb.ai/kemingzhang/vae_dm/runs/01hziqur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/3rtqd2bx3pl1j7g15bs_3jp00000gn/T/ipykernel_98403/21962628.py:44: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert mu.shape == sd.shape\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9efcc98f56e40e6bd5ef511b6c8ca10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.735 MB of 3.251 MB uploaded\\r'), FloatProgress(value=0.8413183093468731, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch_loss</td><td>███▆▅▅▄▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_r2</td><td>▁▄▅▅▅▆▆▆▇▄▇▆▇▆▇▇▇▇▆▇▇▇▆████████▇▆▇▆█▇█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>9e-05</td></tr><tr><td>batch_ct</td><td>15000</td></tr><tr><td>batch_loss</td><td>0.08942</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>test_r2</td><td>0.88614</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-feather-24</strong> at: <a href='https://wandb.ai/kemingzhang/vae_dm/runs/01hziqur' target=\"_blank\">https://wandb.ai/kemingzhang/vae_dm/runs/01hziqur</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240117_200801-01hziqur/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = dataset.genes_in_assay()\n",
    "gene_scores = torch.from_numpy(dataset.dataset[:, 1:].to_numpy().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): MLP(\n",
       "    (relu): ReLU()\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=485, out_features=256, bias=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MLP(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=256, out_features=485, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = model.get_emb(gene_scores).detach().cpu().numpy()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.DataFrame(\n",
    "    data = emb,\n",
    "    index = genes,\n",
    "    columns = [f'EMB_{i}' for i in range(hparams[\"emb_dim\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_0</th>\n",
       "      <th>EMB_1</th>\n",
       "      <th>EMB_2</th>\n",
       "      <th>EMB_3</th>\n",
       "      <th>EMB_4</th>\n",
       "      <th>EMB_5</th>\n",
       "      <th>EMB_6</th>\n",
       "      <th>EMB_7</th>\n",
       "      <th>EMB_8</th>\n",
       "      <th>EMB_9</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_246</th>\n",
       "      <th>EMB_247</th>\n",
       "      <th>EMB_248</th>\n",
       "      <th>EMB_249</th>\n",
       "      <th>EMB_250</th>\n",
       "      <th>EMB_251</th>\n",
       "      <th>EMB_252</th>\n",
       "      <th>EMB_253</th>\n",
       "      <th>EMB_254</th>\n",
       "      <th>EMB_255</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000166136</th>\n",
       "      <td>1.641138</td>\n",
       "      <td>0.113827</td>\n",
       "      <td>3.921674</td>\n",
       "      <td>-1.032297</td>\n",
       "      <td>-1.784951</td>\n",
       "      <td>2.761224</td>\n",
       "      <td>-0.963215</td>\n",
       "      <td>0.054067</td>\n",
       "      <td>0.886144</td>\n",
       "      <td>2.497651</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.551810</td>\n",
       "      <td>-0.874080</td>\n",
       "      <td>0.123632</td>\n",
       "      <td>-1.525758</td>\n",
       "      <td>0.081557</td>\n",
       "      <td>1.207395</td>\n",
       "      <td>0.091882</td>\n",
       "      <td>-3.510774</td>\n",
       "      <td>1.390204</td>\n",
       "      <td>-0.308297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000158497</th>\n",
       "      <td>1.227518</td>\n",
       "      <td>-0.201240</td>\n",
       "      <td>2.679383</td>\n",
       "      <td>0.710508</td>\n",
       "      <td>-1.599195</td>\n",
       "      <td>-0.127268</td>\n",
       "      <td>-0.670575</td>\n",
       "      <td>0.623313</td>\n",
       "      <td>0.310372</td>\n",
       "      <td>1.875062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717993</td>\n",
       "      <td>0.807587</td>\n",
       "      <td>-0.043969</td>\n",
       "      <td>0.827601</td>\n",
       "      <td>-0.581406</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>-0.090785</td>\n",
       "      <td>-0.459049</td>\n",
       "      <td>-0.454707</td>\n",
       "      <td>-0.776513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000130158</th>\n",
       "      <td>-0.305238</td>\n",
       "      <td>-0.931766</td>\n",
       "      <td>2.670334</td>\n",
       "      <td>-1.156555</td>\n",
       "      <td>-0.207786</td>\n",
       "      <td>0.460169</td>\n",
       "      <td>-0.771380</td>\n",
       "      <td>0.193283</td>\n",
       "      <td>0.701356</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092865</td>\n",
       "      <td>0.333991</td>\n",
       "      <td>0.705045</td>\n",
       "      <td>-0.334567</td>\n",
       "      <td>0.143183</td>\n",
       "      <td>0.900249</td>\n",
       "      <td>-2.049591</td>\n",
       "      <td>-1.107022</td>\n",
       "      <td>0.736480</td>\n",
       "      <td>-2.218306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000163513</th>\n",
       "      <td>0.555090</td>\n",
       "      <td>-2.963492</td>\n",
       "      <td>1.886296</td>\n",
       "      <td>-0.194856</td>\n",
       "      <td>-1.277622</td>\n",
       "      <td>0.863973</td>\n",
       "      <td>0.157402</td>\n",
       "      <td>0.730744</td>\n",
       "      <td>0.604816</td>\n",
       "      <td>0.873439</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.013703</td>\n",
       "      <td>-0.266838</td>\n",
       "      <td>-0.954578</td>\n",
       "      <td>0.458120</td>\n",
       "      <td>-0.886794</td>\n",
       "      <td>0.048439</td>\n",
       "      <td>-0.940706</td>\n",
       "      <td>-0.370678</td>\n",
       "      <td>-0.082977</td>\n",
       "      <td>-1.468873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000196735</th>\n",
       "      <td>-0.011198</td>\n",
       "      <td>-1.249279</td>\n",
       "      <td>2.183905</td>\n",
       "      <td>-0.822078</td>\n",
       "      <td>-1.058541</td>\n",
       "      <td>-1.073211</td>\n",
       "      <td>-0.261656</td>\n",
       "      <td>0.421751</td>\n",
       "      <td>0.181857</td>\n",
       "      <td>0.720012</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387877</td>\n",
       "      <td>1.664654</td>\n",
       "      <td>0.181335</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>-1.299922</td>\n",
       "      <td>-0.415443</td>\n",
       "      <td>-0.097746</td>\n",
       "      <td>-1.015305</td>\n",
       "      <td>0.270264</td>\n",
       "      <td>-1.403219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000065911</th>\n",
       "      <td>0.247325</td>\n",
       "      <td>0.267196</td>\n",
       "      <td>1.401120</td>\n",
       "      <td>-0.907286</td>\n",
       "      <td>-0.706039</td>\n",
       "      <td>-0.966901</td>\n",
       "      <td>-0.713497</td>\n",
       "      <td>0.260983</td>\n",
       "      <td>0.510956</td>\n",
       "      <td>1.823110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185108</td>\n",
       "      <td>1.073838</td>\n",
       "      <td>0.564209</td>\n",
       "      <td>1.178624</td>\n",
       "      <td>0.162501</td>\n",
       "      <td>0.113417</td>\n",
       "      <td>0.518227</td>\n",
       "      <td>0.793102</td>\n",
       "      <td>1.065315</td>\n",
       "      <td>-1.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000086159</th>\n",
       "      <td>0.639313</td>\n",
       "      <td>-0.212863</td>\n",
       "      <td>2.246478</td>\n",
       "      <td>-0.773693</td>\n",
       "      <td>-1.101509</td>\n",
       "      <td>-0.425882</td>\n",
       "      <td>-0.530464</td>\n",
       "      <td>0.446519</td>\n",
       "      <td>0.553853</td>\n",
       "      <td>2.969281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321899</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.731371</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>-1.513927</td>\n",
       "      <td>0.211324</td>\n",
       "      <td>-0.295557</td>\n",
       "      <td>-0.082989</td>\n",
       "      <td>0.274435</td>\n",
       "      <td>0.277479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000138964</th>\n",
       "      <td>0.172174</td>\n",
       "      <td>-0.531750</td>\n",
       "      <td>3.232540</td>\n",
       "      <td>0.176913</td>\n",
       "      <td>-1.699616</td>\n",
       "      <td>-1.263994</td>\n",
       "      <td>-0.608762</td>\n",
       "      <td>1.327598</td>\n",
       "      <td>0.686856</td>\n",
       "      <td>1.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405329</td>\n",
       "      <td>0.791420</td>\n",
       "      <td>1.229977</td>\n",
       "      <td>-0.765838</td>\n",
       "      <td>-0.179073</td>\n",
       "      <td>0.456911</td>\n",
       "      <td>-0.385508</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>0.132343</td>\n",
       "      <td>-1.259318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000163510</th>\n",
       "      <td>0.256619</td>\n",
       "      <td>0.799553</td>\n",
       "      <td>1.589412</td>\n",
       "      <td>4.564667</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>-2.206850</td>\n",
       "      <td>3.108769</td>\n",
       "      <td>1.587506</td>\n",
       "      <td>-1.126365</td>\n",
       "      <td>1.531915</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.390278</td>\n",
       "      <td>-3.083746</td>\n",
       "      <td>-4.117815</td>\n",
       "      <td>1.888183</td>\n",
       "      <td>-1.020239</td>\n",
       "      <td>1.859806</td>\n",
       "      <td>2.011737</td>\n",
       "      <td>-0.827389</td>\n",
       "      <td>1.435091</td>\n",
       "      <td>-0.296291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000126091</th>\n",
       "      <td>1.441241</td>\n",
       "      <td>-1.198970</td>\n",
       "      <td>3.401314</td>\n",
       "      <td>0.559480</td>\n",
       "      <td>-2.788920</td>\n",
       "      <td>-0.333662</td>\n",
       "      <td>0.069382</td>\n",
       "      <td>0.756929</td>\n",
       "      <td>0.077742</td>\n",
       "      <td>1.294557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667198</td>\n",
       "      <td>0.299704</td>\n",
       "      <td>0.738592</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>-0.570744</td>\n",
       "      <td>-0.160975</td>\n",
       "      <td>0.616408</td>\n",
       "      <td>0.834823</td>\n",
       "      <td>0.718592</td>\n",
       "      <td>0.586755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16446 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    EMB_0     EMB_1     EMB_2     EMB_3     EMB_4     EMB_5  \\\n",
       "gene_id                                                                       \n",
       "ENSG00000166136  1.641138  0.113827  3.921674 -1.032297 -1.784951  2.761224   \n",
       "ENSG00000158497  1.227518 -0.201240  2.679383  0.710508 -1.599195 -0.127268   \n",
       "ENSG00000130158 -0.305238 -0.931766  2.670334 -1.156555 -0.207786  0.460169   \n",
       "ENSG00000163513  0.555090 -2.963492  1.886296 -0.194856 -1.277622  0.863973   \n",
       "ENSG00000196735 -0.011198 -1.249279  2.183905 -0.822078 -1.058541 -1.073211   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000065911  0.247325  0.267196  1.401120 -0.907286 -0.706039 -0.966901   \n",
       "ENSG00000086159  0.639313 -0.212863  2.246478 -0.773693 -1.101509 -0.425882   \n",
       "ENSG00000138964  0.172174 -0.531750  3.232540  0.176913 -1.699616 -1.263994   \n",
       "ENSG00000163510  0.256619  0.799553  1.589412  4.564667  0.002477 -2.206850   \n",
       "ENSG00000126091  1.441241 -1.198970  3.401314  0.559480 -2.788920 -0.333662   \n",
       "\n",
       "                    EMB_6     EMB_7     EMB_8     EMB_9  ...   EMB_246  \\\n",
       "gene_id                                                  ...             \n",
       "ENSG00000166136 -0.963215  0.054067  0.886144  2.497651  ... -2.551810   \n",
       "ENSG00000158497 -0.670575  0.623313  0.310372  1.875062  ... -0.717993   \n",
       "ENSG00000130158 -0.771380  0.193283  0.701356  0.516930  ...  0.092865   \n",
       "ENSG00000163513  0.157402  0.730744  0.604816  0.873439  ... -1.013703   \n",
       "ENSG00000196735 -0.261656  0.421751  0.181857  0.720012  ... -1.387877   \n",
       "...                   ...       ...       ...       ...  ...       ...   \n",
       "ENSG00000065911 -0.713497  0.260983  0.510956  1.823110  ...  0.185108   \n",
       "ENSG00000086159 -0.530464  0.446519  0.553853  2.969281  ... -0.321899   \n",
       "ENSG00000138964 -0.608762  1.327598  0.686856  1.599976  ...  0.405329   \n",
       "ENSG00000163510  3.108769  1.587506 -1.126365  1.531915  ... -3.390278   \n",
       "ENSG00000126091  0.069382  0.756929  0.077742  1.294557  ... -0.667198   \n",
       "\n",
       "                  EMB_247   EMB_248   EMB_249   EMB_250   EMB_251   EMB_252  \\\n",
       "gene_id                                                                       \n",
       "ENSG00000166136 -0.874080  0.123632 -1.525758  0.081557  1.207395  0.091882   \n",
       "ENSG00000158497  0.807587 -0.043969  0.827601 -0.581406 -0.458845 -0.090785   \n",
       "ENSG00000130158  0.333991  0.705045 -0.334567  0.143183  0.900249 -2.049591   \n",
       "ENSG00000163513 -0.266838 -0.954578  0.458120 -0.886794  0.048439 -0.940706   \n",
       "ENSG00000196735  1.664654  0.181335  0.335060 -1.299922 -0.415443 -0.097746   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000065911  1.073838  0.564209  1.178624  0.162501  0.113417  0.518227   \n",
       "ENSG00000086159  0.013963  0.731371  0.280500 -1.513927  0.211324 -0.295557   \n",
       "ENSG00000138964  0.791420  1.229977 -0.765838 -0.179073  0.456911 -0.385508   \n",
       "ENSG00000163510 -3.083746 -4.117815  1.888183 -1.020239  1.859806  2.011737   \n",
       "ENSG00000126091  0.299704  0.738592  0.636364 -0.570744 -0.160975  0.616408   \n",
       "\n",
       "                  EMB_253   EMB_254   EMB_255  \n",
       "gene_id                                        \n",
       "ENSG00000166136 -3.510774  1.390204 -0.308297  \n",
       "ENSG00000158497 -0.459049 -0.454707 -0.776513  \n",
       "ENSG00000130158 -1.107022  0.736480 -2.218306  \n",
       "ENSG00000163513 -0.370678 -0.082977 -1.468873  \n",
       "ENSG00000196735 -1.015305  0.270264 -1.403219  \n",
       "...                   ...       ...       ...  \n",
       "ENSG00000065911  0.793102  1.065315 -1.298700  \n",
       "ENSG00000086159 -0.082989  0.274435  0.277479  \n",
       "ENSG00000138964  0.206216  0.132343 -1.259318  \n",
       "ENSG00000163510 -0.827389  1.435091 -0.296291  \n",
       "ENSG00000126091  0.834823  0.718592  0.586755  \n",
       "\n",
       "[16446 rows x 256 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.index.name = \"gene_id\"\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = hparams[\"emb_dim\"]\n",
    "emb_df.to_csv(f\"DepMap_unprocessed_vae_d{emb_dim}.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheet5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
